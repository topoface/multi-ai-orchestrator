#!/usr/bin/env python3
"""
Multi-AI Debate Engine
Orchestrates debates between Claude, Gemini, and Perplexity
"""
import sys
import json
import yaml
import os
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime
import anthropic
import requests
from dotenv import load_dotenv
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Import both Vertex AI and Google AI Studio
# Will decide which to use at runtime based on environment
try:
    import vertexai
    from vertexai.generative_models import GenerativeModel as VertexGenerativeModel
    VERTEX_AVAILABLE = True
except ImportError:
    VERTEX_AVAILABLE = False

try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False

# Supabase client (optional, only if configured)
try:
    from supabase import create_client, Client
    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False

# Load environment variables from .env file
env_path = Path(__file__).parent.parent.parent.parent / ".env"
load_dotenv(env_path)

# Load config
config_path = Path(__file__).parent.parent.parent.parent / "config" / "debate_config.yaml"
with open(config_path) as f:
    config = yaml.safe_load(f)

# API Keys
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')  # For Google AI Studio (GitHub Actions)
PERPLEXITY_API_KEY = os.getenv('PERPLEXITY_API_KEY')

# GCP Configuration for Vertex AI (local)
GCP_PROJECT_ID = os.getenv('GCP_PROJECT_ID')
GCP_REGION = os.getenv('GCP_REGION', 'us-central1')

# Determine which Gemini API to use at runtime
# Prefer Vertex AI if available and GCP project is configured
USE_VERTEX_AI = VERTEX_AVAILABLE and GCP_PROJECT_ID is not None

# Supabase (optional)
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_SERVICE_ROLE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')


class DebateEngine:
    def __init__(self, topic: str, expert_mode: bool = False, max_rounds: int = None):
        self.topic = topic
        self.expert_mode = expert_mode
        self.max_rounds = max_rounds or config['debate']['max_rounds']
        self.history: List[Dict[str, Any]] = []

        # Initialize AI clients FIRST
        self.claude_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

        # Initialize Gemini (Vertex AI or Google AI Studio)
        if USE_VERTEX_AI:
            print(f"âœ“ Using Vertex AI (project: {GCP_PROJECT_ID})", file=sys.stderr)
            vertexai.init(project=GCP_PROJECT_ID, location=GCP_REGION)
            self.gemini_model = VertexGenerativeModel(config['participants']['gemini']['model'])
            self.use_vertex = True
        elif GENAI_AVAILABLE:
            print("âœ“ Using Google AI Studio API", file=sys.stderr)
            genai.configure(api_key=GEMINI_API_KEY)
            self.gemini_model = genai.GenerativeModel(config['participants']['gemini']['model'])
            self.use_vertex = False
        else:
            raise ImportError("Neither Vertex AI nor Google AI Studio is available. Install google-cloud-aiplatform or google-generativeai.")

        # Initialize Supabase (optional)
        self.supabase_client = None
        if SUPABASE_AVAILABLE and SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY:
            try:
                self.supabase_client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)
                print("âœ“ Supabase connected", file=sys.stderr)
            except Exception as e:
                print(f"âš  Supabase connection failed: {e}", file=sys.stderr)

        # Fixed expert personas
        self.claude_persona = "ë°˜ë„ì²´, í†µì‹ , ì „ì, ì½”ë”© ë“± ì—”ì§€ë‹ˆì–´ë§ ë¶„ì•¼ ìµœê³  ì „ë¬¸ê°€"
        self.gemini_persona = "ë¬¼ë¦¬, ìˆ˜í•™, í’ˆì§ˆ, í†µê³„ ë“± ì´ë¡ ì— ëŠ¥í†µí•œ ë¦¬ì°¨ë“œ íŒŒì¸ë§Œ"
        self.perplexity_persona = "ë¬¼ë¦¬/ìˆ˜í•™/í’ˆì§ˆ/í†µê³„ ì´ë¡ ê³¼ ë°˜ë„ì²´/í†µì‹ /ì „ì/ì½”ë”© ì—”ì§€ë‹ˆì–´ë§ ëª¨ë‘ì— ì •í†µí•œ ì¤‘ì¬ ì „ë¬¸ê°€"

        print(f"\nğŸ‘¤ ê³ ì • ì „ë¬¸ê°€ ì—­í• :", file=sys.stderr)
        print(f"   Claude: {self.claude_persona}", file=sys.stderr)
        print(f"   Gemini: {self.gemini_persona}", file=sys.stderr)
        print(f"   Perplexity: {self.perplexity_persona}\n", file=sys.stderr)

    def get_claude_response(self, prompt: str, context: str = "", perplexity_feedback: str = "") -> str:
        """Get response from Claude with assigned persona"""
        feedback_section = f"\n\n**Perplexity í”¼ë“œë°±**:\n{perplexity_feedback}" if perplexity_feedback else ""

        system_prompt = f"""ë‹¹ì‹ ì˜ ì—­í• : **{self.claude_persona}**

ğŸ“Œ **ì›ë˜ ì§ˆë¬¸ (ë°˜ë“œì‹œ ì´ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”)**:
"{self.topic}"

ì´ì „ ëŒ€í™” ë‚´ìš©:
{context}{feedback_section}

**ëª©í‘œ**: 3ë¼ìš´ë“œ ë‚´ì— ìƒëŒ€ ì „ë¬¸ê°€({self.gemini_persona})ì™€ **ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ** ì‹¤ìš©ì ì¸ í•©ì˜ì•ˆì„ ë„ì¶œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ì¤‘ìš” ì›ì¹™**:
1. ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ ê´€ì ì—ì„œ ì˜ê²¬ ì œì‹œ
2. ìƒëŒ€ ì „ë¬¸ê°€ì˜ ê´€ì ì„ ì¡´ì¤‘í•˜ê³  ì ˆì¶©ì  ì°¾ê¸°
3. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ ì‘ì„±
4. ê°„ê²°í•˜ê²Œ ì‘ì„± (500-800ì)
5. **ì›ë˜ ì§ˆë¬¸ì—ì„œ ë²—ì–´ë‚˜ì§€ ë§ê³ , ì„¸ë¶€ êµ¬í˜„ë³´ë‹¤ëŠ” í•µì‹¬ ì„ íƒì— ì§‘ì¤‘**
6. **ë°˜ë“œì‹œ í•œê¸€ë¡œ ë‹µë³€**
7. **í•©ì˜í•  ë•ŒëŠ” ëª…í™•í•˜ê²Œ**: "ë™ì˜í•©ë‹ˆë‹¤" ë˜ëŠ” "í•©ì˜ì•ˆì„ ìˆ˜ìš©í•©ë‹ˆë‹¤" ë¼ê³ ë§Œ í•˜ê³  ëë‚´ì„¸ìš”. "ì¢‹ìŠµë‹ˆë‹¤, **í•˜ì§€ë§Œ**..." ì‹ìœ¼ë¡œ ë’¤ì— ìˆ˜ì • ìš”êµ¬ë¥¼ ë¶™ì´ì§€ ë§ˆì„¸ìš”! ê·¸ê±´ ê°€ì§œ í•©ì˜ì…ë‹ˆë‹¤.

**ê²½ê³ **:
- ì—£ì§€ ì¼€ì´ìŠ¤, ë°ì´í„° ê²€ì¦, ë³´ì•ˆ ì„¸ë¶€ì‚¬í•­ ë“± êµ¬í˜„ ë””í…Œì¼ë¡œ ë°œì‚°í•˜ì§€ ë§ˆì„¸ìš”
- "ì¢‹ìŠµë‹ˆë‹¤, í•˜ì§€ë§Œ..." ì‹ì˜ ê°€ì§œ í•©ì˜ ê¸ˆì§€! í•©ì˜í•˜ê±°ë‚˜ ë°˜ëŒ€í•˜ê±°ë‚˜ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ í•˜ì„¸ìš”!

**ë°˜ë“œì‹œ í•œê¸€ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.**"""

        try:
            message = self.claude_client.messages.create(
                model=config['participants']['claude']['model'],
                max_tokens=config['participants']['claude']['max_tokens'],
                temperature=config['participants']['claude']['temperature'],
                system=system_prompt,
                messages=[{"role": "user", "content": prompt}]
            )
            return message.content[0].text

        except Exception as e:
            return f"Error getting Claude response: {e}"

    def get_gemini_response(self, prompt: str, context: str = "", perplexity_feedback: str = "") -> str:
        """Get response from Gemini with assigned persona"""
        feedback_section = f"\n\n**Perplexity í”¼ë“œë°±**:\n{perplexity_feedback}" if perplexity_feedback else ""

        full_prompt = f"""ë‹¹ì‹ ì˜ ì—­í• : **{self.gemini_persona}**

ğŸ“Œ **ì›ë˜ ì§ˆë¬¸ (ë°˜ë“œì‹œ ì´ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•˜ì„¸ìš”)**:
"{self.topic}"

ì´ì „ ëŒ€í™” ë‚´ìš©:
{context}{feedback_section}

{prompt}

**ëª©í‘œ**: 3ë¼ìš´ë“œ ë‚´ì— ìƒëŒ€ ì „ë¬¸ê°€({self.claude_persona})ì™€ **ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ** ì‹¤ìš©ì ì¸ í•©ì˜ì•ˆì„ ë„ì¶œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ì¤‘ìš” ì›ì¹™**:
1. ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ ê´€ì ì—ì„œ ì˜ê²¬ ì œì‹œ
2. ìƒëŒ€ ì „ë¬¸ê°€ì˜ ê´€ì ì„ ì¡´ì¤‘í•˜ê³  ì ˆì¶©ì  ì°¾ê¸°
3. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ ì‘ì„±
4. ê°„ê²°í•˜ê²Œ ì‘ì„± (500-800ì)
5. **ì›ë˜ ì§ˆë¬¸ì—ì„œ ë²—ì–´ë‚˜ì§€ ë§ê³ , ì„¸ë¶€ êµ¬í˜„ë³´ë‹¤ëŠ” í•µì‹¬ ì„ íƒì— ì§‘ì¤‘**
6. **ë°˜ë“œì‹œ í•œê¸€ë¡œ ë‹µë³€**
7. **í•©ì˜í•  ë•ŒëŠ” ëª…í™•í•˜ê²Œ**: "ë™ì˜í•©ë‹ˆë‹¤" ë˜ëŠ” "í•©ì˜ì•ˆì„ ìˆ˜ìš©í•©ë‹ˆë‹¤" ë¼ê³ ë§Œ í•˜ê³  ëë‚´ì„¸ìš”. "ì¢‹ìŠµë‹ˆë‹¤, **í•˜ì§€ë§Œ**..." ì‹ìœ¼ë¡œ ë’¤ì— ìˆ˜ì • ìš”êµ¬ë¥¼ ë¶™ì´ì§€ ë§ˆì„¸ìš”! ê·¸ê±´ ê°€ì§œ í•©ì˜ì…ë‹ˆë‹¤.

**ê²½ê³ **:
- ì—£ì§€ ì¼€ì´ìŠ¤, ë°ì´í„° ê²€ì¦, ë³´ì•ˆ ì„¸ë¶€ì‚¬í•­ ë“± êµ¬í˜„ ë””í…Œì¼ë¡œ ë°œì‚°í•˜ì§€ ë§ˆì„¸ìš”
- "ì¢‹ìŠµë‹ˆë‹¤, í•˜ì§€ë§Œ..." ì‹ì˜ ê°€ì§œ í•©ì˜ ê¸ˆì§€! í•©ì˜í•˜ê±°ë‚˜ ë°˜ëŒ€í•˜ê±°ë‚˜ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ í•˜ì„¸ìš”!

**ë°˜ë“œì‹œ í•œê¸€ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.**"""

        try:
            # Vertex AI SDK uses generation_config as a dict
            response = self.gemini_model.generate_content(
                full_prompt,
                generation_config={
                    'temperature': config['participants']['gemini']['temperature'],
                    'max_output_tokens': config['participants']['gemini']['max_tokens'],
                }
            )
            return response.text

        except Exception as e:
            return f"Error getting Gemini response: {e}"

    def get_perplexity_judgment(self, claude_pos: str, gemini_pos: str) -> Dict[str, Any]:
        """Get judgment from Perplexity on whether consensus is acceptable"""
        if not PERPLEXITY_API_KEY or not config['participants']['perplexity']['enabled']:
            return {"approved": True, "feedback": "Perplexity not available"}

        prompt = f"""ë‹¹ì‹ ì€ {self.perplexity_persona}ë¡œì„œ, ë‘ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ì¤‘ì¬í•˜ê³  í•©ì˜ì— ì´ë¥´ë„ë¡ ë•ëŠ” ì—­í• ì…ë‹ˆë‹¤.

ğŸ“Œ **ì›ë˜ ì§ˆë¬¸**:
"{self.topic}"

ë‹¤ìŒì€ ìœ„ ì§ˆë¬¸ì— ëŒ€í•œ ë‘ ì „ë¬¸ê°€ì˜ ì œì•ˆì…ë‹ˆë‹¤.

**ì „ë¬¸ê°€ A ({self.claude_persona})**:
{claude_pos}

**ì „ë¬¸ê°€ B ({self.gemini_persona})**:
{gemini_pos}

ì¤‘ì¬ìë¡œì„œ ì§ˆë¬¸: ì´ ë‘ ì œì•ˆì´ **ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ** ì‹¤ì§ˆì ì¸ í•©ì˜ì— ë„ë‹¬í–ˆë‚˜ìš”?

ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
DECISION: APPROVE (ë˜ëŠ” REJECT, PARTIAL APPROVE)
REASON: ì´ìœ ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ

í‰ê°€ ê¸°ì¤€:
1. **ì›ë˜ ì§ˆë¬¸ ê´€ë ¨ì„±**: ë‘ ì „ë¬¸ê°€ê°€ ì›ë˜ ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€í•˜ê³  ìˆëŠ”ê°€? (ì—£ì§€ ì¼€ì´ìŠ¤, ë°ì´í„° ê²€ì¦ ë“± êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ìœ¼ë¡œ ë°œì‚°í•˜ì§€ ì•Šì•˜ëŠ”ê°€?)
2. **ì§„ì§œ í•©ì˜ ì—¬ë¶€**: ë‘ ì œì•ˆì´ ì„œë¡œ ì¼ì¹˜í•˜ëŠ”ê°€?
   âš ï¸ **ê°€ì§œ í•©ì˜ ê°ì§€**: ì „ë¬¸ê°€ Bê°€ "ì¢‹ìŠµë‹ˆë‹¤"/"ë™ì˜í•©ë‹ˆë‹¤" ë§í•œ í›„ "**í•˜ì§€ë§Œ**"/"**ê·¸ëŸ¬ë‚˜**"/"**ê°œì„ **"/"**ìˆ˜ì •**"/"**ë³´ì™„**" ë“±ìœ¼ë¡œ ê³„ì† ë°˜ë°•í•˜ë©´ â†’ ì´ê±´ í•©ì˜ê°€ ì•„ë‹™ë‹ˆë‹¤! REJECTí•˜ì„¸ìš”!
3. **ì‹¤í–‰ ê°€ëŠ¥ì„±**: êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œê°€?
4. **í•µì‹¬ ìŸì  í•´ê²°**: ì›ë˜ ì§ˆë¬¸ì˜ í•µì‹¬ ìŸì ì— ê²°ë¡ ì´ ìˆëŠ”ê°€?

**ì¤‘ìš”**:
- ì›ë˜ ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ì„¸ë¶€ êµ¬í˜„ìœ¼ë¡œ ë°œì‚°í•œ ê²½ìš° ë°˜ë“œì‹œ REJECT
- í•œ ìª½ì´ ê³„ì† ìˆ˜ì •/ê°œì„ ì„ ìš”êµ¬í•˜ë©´ í•©ì˜ê°€ ì•„ë‹ˆë¯€ë¡œ REJECT

ì¤‘ì¬ìë¡œì„œ ì–‘ì¸¡ì˜ ì¥ì ì„ ì‚´ë¦¬ë©´ì„œ **ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ** í•©ì˜ì— ì´ë¥´ë„ë¡ íŒë‹¨í•´ì£¼ì„¸ìš”. í•œê¸€ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        try:
            response = requests.post(
                "https://api.perplexity.ai/chat/completions",
                headers={
                    "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": config['participants']['perplexity']['model'],
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": config['participants']['perplexity']['temperature'],
                    "max_tokens": config['participants']['perplexity']['max_tokens']
                },
                timeout=30
            )
            response.raise_for_status()
            result_text = response.json()['choices'][0]['message']['content']

            # Parse decision
            approved = False
            feedback = result_text

            for line in result_text.split('\n'):
                if 'DECISION:' in line:
                    line_upper = line.upper()
                    # Only exact "APPROVE" (not PARTIAL APPROVE, NOT APPROVE, etc.)
                    if 'DECISION: APPROVE' in line_upper or 'DECISION:APPROVE' in line_upper:
                        if 'PARTIAL' not in line_upper and 'NOT' not in line_upper:
                            approved = True
                elif 'REASON:' in line:
                    feedback = line.split('REASON:')[1].strip()

            return {
                "approved": approved,
                "feedback": feedback,
                "full_response": result_text
            }

        except Exception as e:
            print(f"âš  Perplexity íŒì • ì‹¤íŒ¨: {e}", file=sys.stderr)
            return {"approved": True, "feedback": f"Error: {e}"}

        try:
            response = requests.post(
                "https://api.perplexity.ai/chat/completions",
                headers={
                    "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": config['participants']['perplexity']['model'],
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": config['participants']['perplexity']['temperature'],
                    "max_tokens": config['participants']['perplexity']['max_tokens']
                },
                timeout=30
            )
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']

        except Exception as e:
            return f"Error getting Perplexity consensus: {e}"

    def calculate_consensus(self, claude_text: str, gemini_text: str) -> float:
        """Calculate consensus score using TF-IDF and cosine similarity"""
        if not claude_text or not gemini_text:
            return 0.0

        try:
            # Use TF-IDF vectorization with automatic stopword removal
            vectorizer = TfidfVectorizer(
                stop_words='english',
                lowercase=True,
                max_features=500,  # Limit to top 500 terms
                ngram_range=(1, 2),  # Use unigrams and bigrams
                min_df=1
            )

            # Create TF-IDF vectors for both texts
            tfidf_matrix = vectorizer.fit_transform([claude_text, gemini_text])

            # Calculate cosine similarity
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]

            # Ensure result is between 0 and 1
            return max(0.0, min(1.0, similarity))

        except Exception as e:
            # Fallback to simple Jaccard similarity if TF-IDF fails
            print(f"âš ï¸ TF-IDF failed, using Jaccard fallback: {e}", file=sys.stderr)

            claude_words = set(claude_text.lower().split())
            gemini_words = set(gemini_text.lower().split())

            # Remove basic stopwords
            stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be', 'been'}
            claude_words -= stopwords
            gemini_words -= stopwords

            if not claude_words or not gemini_words:
                return 0.0

            intersection = len(claude_words & gemini_words)
            union = len(claude_words | gemini_words)

            return intersection / union if union > 0 else 0.0

    def conduct_debate(self) -> Dict[str, Any]:
        """Conduct debate with Perplexity approval cycles"""
        print(f"\nğŸ¯ ì „ë¬¸ê°€ í† ë¡  ì‹œì‘: {self.topic}\n", file=sys.stderr)
        print(f"ëª©í‘œ: 3ë¼ìš´ë“œ ë‚´ í•©ì˜ ë„ë‹¬ â†’ Perplexity ìŠ¹ì¸\n", file=sys.stderr)

        MAX_CYCLES = 3
        ROUNDS_PER_CYCLE = 3

        context = ""
        claude_final = ""
        gemini_final = ""
        perplexity_feedback = ""
        total_rounds = 0
        approved = False

        for cycle in range(1, MAX_CYCLES + 1):
            print(f"\n{'='*80}", file=sys.stderr)
            print(f"ğŸ“ Cycle {cycle}/{MAX_CYCLES}", file=sys.stderr)
            print(f"{'='*80}\n", file=sys.stderr)

            if cycle > 1:
                print(f"âš ï¸  Perplexity í”¼ë“œë°±: {perplexity_feedback}\n", file=sys.stderr)

            # 3 rounds of discussion per cycle
            for round_num in range(1, ROUNDS_PER_CYCLE + 1):
                total_rounds += 1
                print(f"--- Round {round_num}/3 (Cycle {cycle}) ---\n", file=sys.stderr)

                # Prompt
                if total_rounds == 1:
                    prompt = "ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ ê´€ì ì—ì„œ ì´ ì£¼ì œì— ëŒ€í•œ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
                else:
                    prompt = "ìƒëŒ€ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ê³ ë ¤í•˜ì—¬ í•©ì˜ ê°€ëŠ¥í•œ ì œì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”."

                # Claude's turn
                print(f"ğŸ”µ Claude ({self.claude_persona})...", file=sys.stderr)
                claude_response = self.get_claude_response(prompt, context, perplexity_feedback)
                self.history.append({"cycle": cycle, "round": round_num, "ai": "Claude", "response": claude_response})
                context += f"\n\nClaude (Cycle {cycle}, Round {round_num}):\n{claude_response}"
                claude_final = claude_response

                # Gemini's turn
                print(f"ğŸŸ¢ Gemini ({self.gemini_persona})...\n", file=sys.stderr)
                gemini_response = self.get_gemini_response(prompt, context, perplexity_feedback)
                self.history.append({"cycle": cycle, "round": round_num, "ai": "Gemini", "response": gemini_response})
                context += f"\n\nGemini (Cycle {cycle}, Round {round_num}):\n{gemini_response}"
                gemini_final = gemini_response

            # Perplexity judgment after 3 rounds
            print(f"\nğŸ¯ Perplexity íŒì • ì¤‘...", file=sys.stderr)
            judgment = self.get_perplexity_judgment(claude_final, gemini_final)

            self.history.append({
                "cycle": cycle,
                "round": "judgment",
                "ai": "Perplexity",
                "response": judgment["full_response"]
            })

            if judgment["approved"]:
                print(f"âœ… Perplexity ìŠ¹ì¸! Cycle {cycle}ì—ì„œ í•©ì˜ ì™„ë£Œ.\n", file=sys.stderr)
                approved = True
                break
            else:
                print(f"âŒ Perplexity ê±°ì ˆ", file=sys.stderr)
                print(f"   ì´ìœ : {judgment['feedback']}\n", file=sys.stderr)
                perplexity_feedback = judgment["feedback"]

                if cycle < MAX_CYCLES:
                    print(f"ğŸ”„ Cycle {cycle + 1}ìœ¼ë¡œ ì¬ì‹œë„...\n", file=sys.stderr)

        # Calculate similarity score for reference
        consensus_score = self.calculate_consensus(claude_final, gemini_final)

        # Compile results
        status = "approved" if approved else "max_cycles_reached"
        result = {
            "topic": self.topic,
            "timestamp": datetime.utcnow().isoformat(),
            "cycles": cycle,
            "total_rounds": total_rounds,
            "consensus_score": consensus_score,
            "status": status,
            "perplexity_approved": approved,
            "history": self.history,
            "claude_persona": self.claude_persona,
            "gemini_persona": self.gemini_persona,
            "claude_final_position": claude_final,
            "gemini_final_position": gemini_final,
            "perplexity_final_judgment": judgment["full_response"]
        }

        # Save to Supabase (if available)
        if self.supabase_client:
            self.save_to_supabase(result)

        return result

    def save_to_supabase(self, result: Dict[str, Any]) -> None:
        """Save debate result to Supabase"""
        if not self.supabase_client:
            return

        try:
            data = {
                'topic': self.topic,
                'claude_position': result['claude_final_position'],
                'gemini_position': result['gemini_final_position'],
                'consensus_score': result['consensus_score'],
                'rounds': result['total_rounds'],
                'metadata': {
                    'timestamp': result['timestamp'],
                    'status': result['status'],
                    'cycles': result['cycles'],
                    'perplexity_approved': result['perplexity_approved'],
                    'claude_persona': result['claude_persona'],
                    'gemini_persona': result['gemini_persona'],
                    'rounds_detail': result['history'],
                    'perplexity_judgment': result.get('perplexity_final_judgment'),
                    'expert_mode': self.expert_mode
                }
            }

            response = self.supabase_client.table('debate_results').insert(data).execute()
            if response.data:
                print(f"âœ“ Supabase: Debate saved (ID: {response.data[0]['id']})", file=sys.stderr)
        except Exception as e:
            print(f"âš  Supabase save failed: {e}", file=sys.stderr)


def format_result(result: Dict[str, Any]) -> str:
    """Format debate result for display"""
    approval_status = "âœ… ìŠ¹ì¸ë¨" if result['perplexity_approved'] else "âš ï¸ ìµœëŒ€ ì‚¬ì´í´ ë„ë‹¬"

    output = [
        f"\n{'='*80}",
        f"ğŸ¯ ì „ë¬¸ê°€ í† ë¡  ê²°ê³¼: {result['topic']}",
        f"{'='*80}\n",
        f"Timestamp: {result['timestamp']}",
        f"Cycles: {result['cycles']}",
        f"Total Rounds: {result['total_rounds']}",
        f"Similarity Score: {result['consensus_score']:.2%}",
        f"Perplexity íŒì •: {approval_status}",
        f"Status: {result['status'].upper()}\n",
        f"{'='*80}",
        f"\n## ğŸ‘¤ ì „ë¬¸ê°€ A: {result['claude_persona']}\n",
        result['claude_final_position'],
        f"\n{'='*80}",
        f"\n## ğŸ‘¤ ì „ë¬¸ê°€ B: {result['gemini_persona']}\n",
        result['gemini_final_position'],
    ]

    if result.get('perplexity_final_judgment'):
        output.extend([
            f"\n{'='*80}",
            "\n## ğŸ¯ Perplexity ìµœì¢… íŒì •\n",
            result['perplexity_final_judgment']
        ])

    output.append(f"\n{'='*80}\n")

    return "\n".join(output)


def save_result(result: Dict[str, Any], output_path: Path = None):
    """Save debate result to files"""
    brain_dir = Path(__file__).parent.parent.parent.parent / "docs" / "brain"
    brain_dir.mkdir(parents=True, exist_ok=True)

    # Save JSON
    json_path = output_path or brain_dir / f"debate_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_path, 'w') as f:
        json.dump(result, f, indent=2)

    # Append to DECISIONS.md
    decisions_path = brain_dir / "DECISIONS.md"
    with open(decisions_path, 'a') as f:
        f.write(f"\n\n## Decision: {result['topic']}\n")
        f.write(f"**Date**: {result['timestamp']}\n")
        f.write(f"**Consensus**: {result['consensus_score']:.2%}\n")
        f.write(f"**Status**: {result['status']}\n\n")
        f.write(f"**Final Decision**:\n{result['claude_final_position'][:500]}...\n")
        f.write(f"\nFull details: [debate_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json](debate_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json)\n")


def main():
    if len(sys.argv) < 2:
        print("Usage: debate_engine.py <topic> [--expert] [--quick]", file=sys.stderr)
        sys.exit(1)

    # Parse arguments
    args = sys.argv[1:]
    expert_mode = "--expert" in args
    quick_mode = "--quick" in args
    output_path = None

    if "--output" in args:
        idx = args.index("--output")
        if idx + 1 < len(args):
            output_path = Path(args[idx + 1])
            args = args[:idx] + args[idx + 2:]

    # Remove flags from topic
    topic = " ".join([arg for arg in args if not arg.startswith("--")])

    # Set rounds
    max_rounds = 2 if quick_mode else config['debate']['max_rounds']

    try:
        # Conduct debate
        engine = DebateEngine(topic, expert_mode, max_rounds)
        result = engine.conduct_debate()

        # Format and print
        output = format_result(result)
        print(output)

        # Save results
        save_result(result, output_path)
        print(f"\nâœ“ Results saved to docs/brain/", file=sys.stderr)

    except Exception as e:
        print(f"Debate error: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
